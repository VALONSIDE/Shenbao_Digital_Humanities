"""
Module: Visualization (Word Cloud)
Description: Generates word clouds for different ad categories.
             Includes Traditional-to-Simplified Chinese conversion and EXTENSIVE stopword filtering.
             (Fixed Version: Handles CSV parsing errors)
"""

import pandas as pd
import jieba
from wordcloud import WordCloud
import re
import zhconv  # Library for Traditional -> Simplified Chinese conversion
import os

# --- Configuration ---
# è‡ªåŠ¨å‘ä¸Šå¯»æ‰¾ data æ–‡ä»¶å¤¹
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_FILE = os.path.join(BASE_DIR, "data", "encoded_ads.csv")
OUTPUT_DIR = os.path.join(BASE_DIR, "output")

# Windows å­—ä½“è·¯å¾„
FONT_PATH = "C:/Windows/Fonts/simhei.ttf"

# ================= SUPER STOPWORD LIST =================
STOPWORDS = {
    # 1. Basic Function Words
    'ä¹‹', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å¤§', 'åŠ', 'ä¸', 'ç­‰', 'æˆ–', 'æ­¤', 'äº¦', 'å³',
    'æˆ‘ä»¬', 'å¯ä»¥', 'è¿™ä¸ª', 'ä¸€ä¸ª', 'ä»·', 'å…ƒ', 'å·', 'è·¯', 'æˆ¿', 'è¯', 'éƒ¨', 'å¤„', 'ä¸º', 'ä»¥',
    'æ³¨æ„', 'åŠæ³•', 'æ— ä¸', 'ä¸è®º', 'ä¸€åˆ‡', 'å„ç§', 'ä¸€ç§', 'äºŒç§', 'ä¸‰ç§', 'å‡ ç§', 'å› ä¸º', 'æ‰€ä»¥',
    'è®¸å¤š', 'å¸¸å¸¸', 'éå¸¸', 'ååˆ†', 'æ¯”è¾ƒ', 'ä¸è¿‡', 'ä½†æ˜¯', 'è‹¥æ˜¯', 'æˆ–è€…', 'ä»¥åŠ',
    
    # 2. Commercial Noise & Location/Time
    'ä¸Šæµ·', 'ä¸Šæµ·å¸‚', 'ç”³æŠ¥', 'å¹¿å‘Š', 'å‘è¡Œ', 'æ€»', 'åˆ†', 'æ´‹è¡Œ', 'å…¬å¸', 'å¯', 'è°¨å¯', 'å¯äº‹',
    'ç”µè¯', 'åœ°å€', 'ç»ç†', 'å”®', 'è¯•æœ', 'ä»£å”®', 'å‡ºå”®', 'åˆ¶é€ ', 'å‡ºå“', 'åˆ›åˆ¶', 'è¯¸å›',
    'ä¸€å¾‹', 'åŒæ—¶', 'èµ å“', 'å¤§èµ å“', 'å…è´¹', 'å‡½ç´¢', 'ç®€ç« ', 'ç´¢å¯„', 'æ ·æœ¬', 'å¤§å»‰', 'ä»·ç›®',
    'è€ç‰Œ', 'åç‰Œ', 'å”¯ä¸€', 'ç¬¬ä¸€', 'æ— ä¸Š', 'æœ€é«˜', 'æœ€ä¼˜', 'ç‰¹åˆ«', 'ç‰¹æ®Š', 'æœ‰å', 'è‘—å',
    'æ—¥', 'æœˆ', 'å¹´', 'æ˜ŸæœŸ', 'ç¤¼æ‹œ', 'åäºŒæœˆ', 'åä¸€æœˆ', 'ä¸€æœˆ', 'äºŒæœˆ', 'ä¸‰æœˆ', 'å››æœˆ', 'äº”æœˆ',
    'å»¿æ—¥', 'æœ¬æ—¥', 'ç°åœ¨', 'å†¬å­£', 'å†¬ä»¤', 'æ–°æ˜¥', 'å¼€å¹•', 'ä¸¾è¡Œ', 'æœ¬åŸ ', 'å¤–åŸ ',
    'å‘¨å¹´', 'çºªå¿µ', 'å¤§å‡ä»·', 'å»‰ä»·', 'ä¼˜å¾…', 'æœ¬æ ¡', 'æœ¬é™¢', 'æœ¬ç¤¾',
    
    # 3. Education Specific Noise
    'å‡½æˆ', 'å­¦æ ¡', 'å­¦ç¤¾', 'ä¹¦å±€', 'å°ä¹¦é¦†', 'ä¸­åä¹¦å±€', 'å•†åŠ¡å°ä¹¦é¦†', 'å•†åŠ¡', 'å¤§ä¸œä¹¦å±€',
    'æ•™è‚²', 'å±€', 'é¦†', 'æ‰€', 'ç¤¾', 'ç§‘', 'çº§', 'å‘˜', 'ç”Ÿ', 'å¸ˆ', 'ç§ç«‹', 'å…¬ç«‹',
    'æ‹›ç”Ÿ', 'æ‹›æ”¶', 'é™„è®¾', 'å¼€è®¾', 'å¼€å­¦', 'æŠ¥å', 'é€šå‘Š', 'ç« ç¨‹', 'ç®€ç« ', 'æ–°ç”Ÿ', 'æ³¨å†Œ',
    'æ¯•ä¸š', 'è‚„ä¸š', 'ä¸“ä¿®', 'è®²ä¹‰', 'å­¦è´¹', 'æ•™æˆ', 'åŒå­¦', 'å¤§å­¦',
    
    # 4. Beauty/Health Specific Noise
    'ç¾å®¹', 'ç¾å®¹å“', 'ç¾å®¹é™¢', 'è¡¥è„‘', 'è„‘æ±', 'è¡¥è¡€', 'ç¥ç»è¡°å¼±', 'è¡°å¼±', 'ç¥ç»',
    'è‰¾ç½—', 'è‰¾ç½—è¡¥', 'ä¸­æ³•', 'å¤§è¯æˆ¿', 'è¯æˆ¿', 'äº”æ´²', 'å…ˆæ–½', 'é›…éœœ', 'éŸ¦å»‰å£«', 'å®‰ç¥ºå„¿',
    'å¥‡è¯', 'å¦™å“', 'åœ£è¯', 'çµè¯', 'ç‰¹æ•ˆ', 'åŠŸæ•ˆ', 'åŠŸèƒ½', 'æ•ˆåŠ›', 'è‰¯è¯', 'å¤§è¡¥', 'è¡¥å‰‚',
    'åº”ç”¨', 'ç§˜è¯€', 'ç§˜å¯†', 'æ³•', 'å‰‚', 'ä¸¸', 'æ°´', 'è†', 'æ²¹', 'éœ²', 'ç‰‡', 'å‡ è®¸',
    'å¼ºèº«', 'å¥ä½“', 'å«ç”Ÿ', 'æ»‹è¡¥', 'æœç”¨', 'ç²¾åˆ¶', 'æ”¹è‰¯', 'å‘æ˜', 'ä¿å«', 'æ•‘æ˜Ÿ', 'äººä¸¹'
}

def generate_wordcloud(category, texts):
    """
    Generates and saves a word cloud image.
    """
    print(f"[*] Processing WordCloud for: {category}")
    
    # 1. Convert Traditional to Simplified Chinese
    texts_simp = [zhconv.convert(str(t), 'zh-cn') for t in texts]
    
    # 2. Tokenization and Cleaning
    full_text = " ".join(texts_simp)
    full_text = re.sub(r"[^\u4e00-\u9fa5]", "", full_text) 
    
    words = jieba.lcut(full_text)
    
    clean_words = [w for w in words if len(w) > 1 and w not in STOPWORDS]
    
    if not clean_words:
        print(f"    [!] No valid words found for {category} after cleaning.")
        return

    # 3. Color Scheme
    if 'Beauty' in category or 'ç¾å®¹' in str(category):
        c_map = 'magma'
    elif 'Education' in category or 'å‡½æˆ' in str(category):
        c_map = 'cividis'
    else:
        c_map = 'viridis'

    # 4. Rendering
    wc = WordCloud(
        font_path=FONT_PATH,
        width=1600, height=1000,
        background_color='white',
        max_words=100,
        colormap=c_map,
        prefer_horizontal=0.9,
        random_state=42
    ).generate(" ".join(clean_words))
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    output_path = os.path.join(OUTPUT_DIR, f"wordcloud_{category}.png")
    wc.to_file(output_path)
    print(f"    [+] Saved: {output_path}")

def main():
    print(f"[*] Reading data from: {DATA_FILE}")
    if not os.path.exists(DATA_FILE):
        print(f"[!] Error: Data file not found. Please run '2_data_coding.py' first.")
        return

    # === å…³é”®ä¿®æ”¹ï¼šå¢åŠ å®¹é”™å‚æ•° ===
    try:
        df = pd.read_csv(DATA_FILE, encoding='utf-8-sig', on_bad_lines='skip', engine='python')
    except UnicodeDecodeError:
        print("[!] UTF-8 failed, trying GBK...")
        df = pd.read_csv(DATA_FILE, encoding='gbk', on_bad_lines='skip', engine='python')
    except Exception as e:
        print(f"[!] Critical Read Error: {e}")
        return
    
    print(f"[*] Successfully loaded {len(df)} records.")

    # Detect column names (Handling potential Chinese/English headers)
    if 'Category' in df.columns:
        cat_col = 'Category'
        text_col = 'å®Œæ•´æ ‡é¢˜' # Assuming standard output from step 2
    elif 'å…³é”®è¯' in df.columns:
        cat_col = 'å…³é”®è¯'
        text_col = 'å®Œæ•´æ ‡é¢˜'
    else:
        print(f"[!] Error: Could not find Category column. Available columns: {df.columns}")
        return

    categories = df[cat_col].unique()
    
    for cat in categories:
        subset = df[df[cat_col] == cat]
        
        if text_col in subset.columns:
            titles = subset[text_col].tolist()
            generate_wordcloud(cat, titles)
        else:
            print(f"[!] Error: '{text_col}' column missing.")

    print("\nğŸ‰ All Word Clouds Generated in 'output/' folder!")

if __name__ == "__main__":
    main()